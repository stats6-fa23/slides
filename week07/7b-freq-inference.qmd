---
title: "Introduction to Frequentist Inference"
subtitle: "Inference for a Proportion"
author: "Dr. Mine Dogucu"
execute:
  echo: false
format: 
  revealjs:
    footer: "[introdata.science](https://introdata.science)"
    slide-number: true
    incremental: true
    theme: ["../templates/slide-style.scss"]
    logo: "https://www.introdata.science/img/logo.png"
    title-slide-attributes: 
      data-background-image: "https://www.introdata.science/img/logo.png"
      data-background-size: 5%
      data-background-position: 50% 85%
    include-after-body: "../templates/clean_title_page.html"
---

## 

```{r}
options(scipen=0)
library(bayesrules)
library(tidyverse)
library(scales)
```

```{r}
#| echo: false
theme_set(theme_bw(base_size = 18))
```


# Central Limit Theorem Review

## Remembering CLT


```{r fig.height = 4, fig.align='center'}
set.seed(7)

bike_prop <- 0.15

bike <- c(rep("yes", bike_prop*10000), rep("no", (1-bike_prop)*10000))

pop <- tibble(bike)


pop %>% 
  ggplot() +
  aes(x = bike, y = ..prop.., group = 1) +
  geom_bar() +
  labs(title = "Population Distribution")


```

Let $\pi$ represent the proportion of bike owners on campus then $\pi =$ `r bike_prop`. 


## Getting to sampling distribution of a proportion

$p_1$ - Proportion of first sample (n = 100)

```{r}
sample1 <- pop %>% 
  sample_n(100) %>% 
  count(bike) %>% 
  mutate(prop = n/sum(n)) %>% 
  filter(bike == "yes") %>% 
  select(prop) %>% 
  pull()

sample1
```

$p_2$ -Proportion of second sample (n = 100)

```{r}
pop %>% 
  sample_n(100) %>% 
  count(bike) %>% 
  mutate(prop = n/sum(n)) %>% 
  filter(bike == "yes") %>% 
  select(prop) %>% 
  pull()
```

$p_3$ -Proportion of third sample (n = 100)

```{r}
pop %>% 
  sample_n(100) %>% 
  count(bike) %>% 
  mutate(prop = n/sum(n)) %>% 
  filter(bike == "yes") %>% 
  select(prop) %>% 
  pull()
```

##

```{r cache = TRUE}

sample_props <- vector()


for (i in 1:10000){
 
    sample <- pop %>% 
    sample_n(100) 
    
     sample_props[i] <- min(prop.table(table(sample$bike)))
  
  
  
}

sample_props_data <- tibble(sample_props = sample_props)
```



## Sampling Distribution of a Proportion

```{r}

sample_props_data %>% 
  ggplot() +
  aes(x = sample_props) +
  geom_histogram(binwidth=0.01) +
  geom_vline(xintercept = bike_prop, color= "#e56646") +
  labs(x = "Sample proportions")
```

##

If certain conditions are met then

$$p \sim \text{approximately } N(\pi, \frac{\pi(1-\pi)}{n})$$

# Confidence Interval for a Proportion

## In Reality

- We only have one sample and thus one point estimate of the population parameter. How can make use of it? 

. . .

- First we will assume the sample proportion is the best thing we have at hand and use it as a point estimate of the population proportion. 

. . .

- Second, even though we embrace the sample proportion as a point estimate of the population proportion, we will need to acknowledge that it has some error. 



## Standard Error


$p \sim  \text{approximately } N(\text{mean} = \pi, \text{sd} = \sqrt{\frac{\pi(1-\pi)}{n}})$

. . .

We call the standard deviation of the sampling distribution __standard error__ of the estimate. 

Standard error of a proportion is $\sqrt{\frac{p(1-p)}{n}}$.



## Confidence Interval 

CI = $\text{point estimate} \pm \text { margin of error}$

. . .

CI = $\text{point estimate} \pm \text { critical value} \times \text{standard error}$

. . .

CI for a proportion = $p \pm \text {critical value} \times \text{standard error}$

. . .

CI for a proportion = $p \pm \text {critical value} \times \sqrt{\frac{p(1-p)}{n}}$

. . .

95% CI for a proportion = $p \pm 1.96 \times \sqrt{\frac{p(1-p)}{n}}$ because ...

##

95% of the data falls within 1.96 standard deviations in the normal distribution.
      
```{r}

z <- seq(-4, 4, by = 0.01)
y <- dnorm(z)
data <- tibble(z = z, y = y)
qplot(z, y, data = data, 
          geom = "line") +
  geom_ribbon(data = subset(data, z > -1.96 & z <1.96),
                  aes(ymax = y), 
                  ymin = 0,
                  fill = "#e56646", 
                  colour = NA, 
                  alpha = 0.5) +
  ylab("") +
  scale_x_continuous(breaks = c(-1.96,0,1.96)) +
  annotate(geom = "text", x = 0, y = 0.15, label = "95%") +
  annotate(geom = "text", x = 2.3, y = 0.01, label = "2.5%") +
  annotate(geom = "text", x = - 2.3, y = 0.01, label = "2.5%")

  
```
      

## How do we know that?

```{r echo = TRUE}
qnorm(0.025, mean = 0 , sd = 1)
```

```{r echo = TRUE}
qnorm(0.975, mean = 0 , sd = 1)
```


## 95% CI for the first sample

Recall $p = 0.17$ and $n = 100$

. . .

95% CI for a proportion = $p \pm 1.96 \times \sqrt{\frac{p(1-p)}{n}}$ 

. . .

95% CI = $0.17 \pm 1.96 \times \sqrt{\frac{0.17(1-0.17)}{100}}$ 

. . .

95% CI = $0.17 \pm 1.96 \times 0.03756328$ 

. . .

95%CI = $0.17 \pm 0.07362403$

. . .

95%CI = (0.09637597, 0.243624)


## 95% CI for the first sample

95%CI = (0.09637597, 0.243624)

We are 95% confident that the true population proportion of bike owners is in this confidence interval.


95%CI = (0.09637597, 0.243624)


```{r}
tibble(p = bike_prop, n = 1, lower_bound = 0.09637597, upper_bound = 0.243624) %>% 
  ggplot() +
  aes(x = 0.17, y = n) +
  geom_point() +
  geom_errorbarh(aes(xmin = lower_bound, xmax = upper_bound, height = .01)) +
  labs(y = " ",
       x = " ") +
  theme(axis.text.y = element_blank()) 
```


## Understanding Confidence Intervals


I have taken 100 samples with $n = 100$, calculated the sample proportion, standard error, and 95% CI interval for each sample

```{r}
set.seed(26)

cv <- qnorm(0.025)

ci_data <- sample_props_data %>% 
  sample_n(100) %>% 
  rename(p = sample_props) %>% 
  mutate(SE = sqrt((p*(1-p))/100),
         lower_bound = p + (cv*SE),
         upper_bound = p  - (cv*SE))


ci_data
```


## Understanding Confidence Intervals

```{r}
ci_data <-
  ci_data %>% 
  mutate(n = 1:nrow(ci_data)) %>% 
  mutate(contains = ifelse(lower_bound < bike_prop & upper_bound > bike_prop , 1,0)) 
```


```{r}
ci_data %>% 
  ggplot() +
  aes(x = p, y = n) +
  geom_point() +
  geom_errorbarh(aes(xmin = lower_bound, xmax = upper_bound)) +
  labs(y = " ") +
  theme(axis.text.y = element_blank()) 
```


## Understanding Confidence Intervals


```{r}
ci_data %>% 
  ggplot() +
  aes(x = p, y = n, color = as.factor(contains)) +
  geom_point() +
  geom_errorbarh(aes(xmin = lower_bound, xmax = upper_bound)) +
  labs(y = " ") +
  theme(axis.text.y = element_blank(),
        legend.position = "none")  +
  geom_vline(xintercept =  bike_prop, color= "#e56646") 
  
```


## Confidence Interval Width

Which of the following confidence intervals would be the widest? Why?

- 90% CI
- 95% CI
- 99% CI

##

CI = $\text{point estimate} \pm \text { critical value} \times \text{standard error}$


```{r echo = TRUE}
qnorm(0.05, mean = 0, sd = 1) # critical value for 90%CI
```


```{r fig.height=4}

z <- seq(-4, 4, by = 0.01)
y <- dnorm(z)
data <- tibble(z = z, y = y)
qplot(z, y, data = data, 
          geom = "line") +
  geom_ribbon(data = subset(data, z > -1.64 & z <1.64),
                  aes(ymax = y), 
                  ymin = 0,
                  fill = "#e56646", 
                  colour = NA, 
                  alpha = 0.5) +
  ylab("") +
  scale_x_continuous(breaks = c(-1.64,0,1.64)) +
  annotate(geom = "text", x = 0, y = 0.15, label = "90%") +
  annotate(geom = "text", x = 2, y = 0.03, label = "5%") +
  annotate(geom = "text", x = - 2, y = 0.03, label = "5%")

  
```

##

CI = $\text{point estimate} \pm \text { critical value} \times \text{standard error}$


```{r echo = TRUE}
qnorm(0.025, mean = 0, sd = 1) # critical value for 95%CI
```


```{r fig.height=4}

z <- seq(-4, 4, by = 0.01)
y <- dnorm(z)
data <- tibble(z = z, y = y)
qplot(z, y, data = data, 
          geom = "line") +
  geom_ribbon(data = subset(data, z > -1.96 & z <1.96),
                  aes(ymax = y), 
                  ymin = 0,
                  fill = "#e56646", 
                  colour = NA, 
                  alpha = 0.5) +
  ylab("") +
  scale_x_continuous(breaks = c(-1.96,0,1.96)) +
  annotate(geom = "text", x = 0, y = 0.15, label = "95%") +
  annotate(geom = "text", x = 2.23, y = 0.01, label = "2.5%") +
  annotate(geom = "text", x = - 2.23, y = 0.01, label = "2.5%")

  
```

##

CI = $\text{point estimate} \pm \text { critical value} \times \text{standard error}$


```{r echo = TRUE}
qnorm(0.005, mean = 0, sd = 1) # critical value for 99%CI
```


```{r fig.height=4}

z <- seq(-4, 4, by = 0.01)
y <- dnorm(z)
data <- tibble(z = z, y = y)
qplot(z, y, data = data, 
          geom = "line") +
  geom_ribbon(data = subset(data, z > -2.58 & z <2.58),
                  aes(ymax = y), 
                  ymin = 0,
                  fill = "#e56646", 
                  colour = NA, 
                  alpha = 0.5) +
  ylab("") +
  scale_x_continuous(breaks = c(-2.58,0,2.58)) +
  annotate(geom = "text", x = 0, y = 0.15, label = "99%")

  
```

##

CI = $\text{point estimate} \pm \text { critical value} \times \text{standard error}$

- 99% CI has the highest critical value.
- Higher critical value means higher margin of error.
- Higher margin of error means wider CI.

Thus 99% CI would be the widest.


##

[Garfield](https://www.gocomics.com/garfield/1999/03/12)



## Effect of Sample Size on Confidence Interval 

Researchers A, B, and C are interested in proportion of bike ownership took samples. They each take separate samples of size 100, 500, and 1000 respectively. They each have a sample proportion of 0.18. What a surprise! Which of the researchers will find the narrowest 95% CI?

##

**Researcher A:** 95% CI for a proportion = $0.18 \pm 1.96 \times \sqrt{\frac{0.18(1-0.18)}{100}}$ 

**Researcher B:** 95% CI for a proportion = $0.18 \pm 1.96 \times \sqrt{\frac{0.18(1-0.18)}{500}}$ 

**Researcher C:** 95% CI for a proportion = $0.18 \pm 1.96 \times \sqrt{\frac{0.18(1-0.18)}{1000}}$ 

. . .

As sample size increases, the standard error decreases and the margin of error also decreases, thus the confidence interval interval gets narrower. The Researcher C would have the narrowest CI.


## CLT

If these conditions are met then

$p \sim \text{approximately } N(\pi, \frac{\pi(1-\pi)}{n})$


## Conditions


1. The sample data are independent. 
2. There needs to be at least 10 successes and 10 failures in the sample.
3. The sample size is smaller than 10% of the population.

##

__Example__

According to a Gallup Survey  of 1017 adults living in US 66% of Americans favor legalizing marijuana. 

Compute 95% confidence interval for the population proportion of those who favor legalizing marijuana. 

Information on the survey can be found [here](https://news.gallup.com/poll/267698/support-legal-marijuana-steady-past-year.aspx)



## Checking Conditions

$n = 1017$ and $p = 0.66$

1) The survey link indicates that the respondents were chosen from a random sample. We would expect such sample to be independent.   

. . .

2) We need at least 10 people favoring legalizing marijuana and 10 people opposing this. 

$np = 1017 \cdot 0.66 = 671.22$. There are more than 10 people favoring legalizing marijuana.
$n(1-p) = 1017 \cdot (1-0.66) = 345.78$. There are more than 10 people opposing legalizing marijuana.

. . .

##


3) 1017 is less than 10% of US population.


## Confidence Interval 

CI = $\text{point estimate} \pm \text { margin of error}$

. . .

CI = $\text{point estimate} \pm \text { critical value} \times \text{standard error}$

. . .

95% CI for a proportion = $p \pm 1.96 \times \sqrt{\frac{p(1-p)}{n}}$ 

. . .

95% CI = $0.66 \pm 1.96 \times \sqrt{\frac{0.66(1-0.66)}{1017}}$ 

. . .

95% CI = (0.6308857, 0.6891143)

We are 95% confident that the true proportion of Americans who support legalizing marijuana falls between 0.6308857 and 0.6891143.



## Confidence Interval Using R

```{r}
#| echo: true
p <- 0.66 #sample proportion
n <- 1017 #sample size

se <- sqrt(p*(1-p)/n) #standard error
cv <- qnorm(0.975) #critical value

p - cv*se #lower bound of the CI
p + cv*se #upper bound of the CI
```


# Hypothesis Testing for a Proportion



## Review of Hypothesis Testing

- We always assume the null hypothesis is true at the beginning.

. . .

- We look for evidence against the null.

. . . 

- If we find any evidence against the null (e.g. a single pink cow) then we can conclude null is false. We say we **reject the null hypothesis**.

. . .

##

- If we do not find any evidence against the null (a single pink cow) then we fail to reject the null. We can keep searching for more evidence against the null (i.e. continue looking for a pink cow). We will never be able to say the null is true so we never accept the null. We **fail to reject the null**. All we can do is keep looking for a pink cow.



- We are searching for evidence against the null. 
We are searching for evidence in the sample that may serve as an evidence against the null. 





## Research Question

Do the majority of Americans approve allowing DACA immigrants to become citizens?


## Hypotheses 

$H_0: \pi \leq 0.5$  
$H_A: \pi > 0.5$



## Assuming $H_0$ is True

Recall that according to CLT $p \sim \text{approximately }N(\pi, \frac{\pi(1-\pi)}{n})$

. . .

If $H_0: \pi = 0.5$ then the null sampling distribution would be $N(0.5, \frac{0.5(1-0.5)}{n})$


## Looking for Evidence 


According to a Gallup survey of 1520 US adults , 83% approve of allowing DACA immigrants to become citizens.



. . .

$p = 0.83$  
$n = 1520$

. . .

We said that the null sampling distribution would be $N(0.5, \frac{0.5(1-0.5)}{n})$ which is

$N(0.5, \frac{0.5(1-0.5)}{1520})$

$N(0.5, 0.0001644737)$


## The $H_0$ Sampling Distribution 

```{r}
p <- seq(0.45, 0.55, by = 0.0001)
y <- dnorm(p, mean = 0.5, sd = sqrt(0.5*0.5/1520))
data <- tibble(p = p, y = y)
qplot(p, y, data = data, 
          geom = "line") +
    ylab("") 


```


## What counts as evidence against the null?

Any sample proportion that falls of really far away from the center of the distribution would count as an evidence against the null.

If the null is true, then it would be unlikely to observe extremely high or low sample proportions. 


## Sampling Distribution

```{r}

se <- sqrt(0.5*0.5/1520)
p <- seq(0.1, 0.9, by = 0.0001)
y <- dnorm(p, mean = 0.5, sd = se)
data <- tibble(p = p, y = y)
qplot(p, y, data = data, 
          geom = "line") +
  ylab("") +
    geom_vline(xintercept = 0.83) +
   annotate(geom = "text", x = 0.78, y = 20, label = "p = 0.83")


```

##


We want to know the probability of observing an extreme sample proportion like ours (p = 0.83) if the $H_0$ were true.

. . .

If our sample proportion is "extreme" then so is 0.90, 0.91, 0.917, 0.9273423 etc. 




##


If the $H_0$ is true what is the probability that we will observe an extremely high sample proportion?


Probability of observing sample proportion of 0.83 and higher 

```{r echo=TRUE}
pnorm(0.83, mean = 0.5, sd = 0.01282473, 
      lower.tail = FALSE)
```

P-value is the probability of observing a sample statistic at least as extreme as the one that has been observed if the null hypothesis were true.

. . .

If p-value is less than 0.05 we reject the null hypothesis. 


## Decision and Conclusion

In the Gallup survey the sample proportion was 0.83. 

. . .

If the null hypothesis (
$H_0 = \pi = 0.5$
) were true, then it would be quite unlikely (p = $2.594241 \times 10^{-146}$) to observe a sample proportion that is at least extreme like the one we observed. We consider this "quite unlikely" since p < 0.05.


. . .

Since we have observed the sample proportion of 0.83 then we reject the $H_0$.

. . .

We conclude that the population proportion of Americans who approve allowing DACA immigrants to become citizens is higher than 0.5.


## Conditions

`r fontawesome::fa(name = "check", fill = "#e56646")` [Gallup website](https://news.gallup.com/poll/235775/americans-oppose-border-walls-favor-dealing-daca.aspx) indicates that the sample was randomly selected. We will assume independence.

. . .

:::{.pull-left}

`r fontawesome::fa(name = "check", fill = "#e56646")`Number of successes > 10

```{r echo = TRUE}
0.83*1520
```

:::

:::{.pull-right}

`r fontawesome::fa(name = "check", fill = "#e56646")`Number of failures > 10

```{r echo = TRUE}
(1-0.83)*1520
```

:::

. . .

<br>
<br>
<br>
<br>

`r fontawesome::fa(name = "check", fill = "#e56646")`1520 < 10% US Population

## Steps of Hypotheses Testing

1. Set hypotheses
2. Identify Sampling Distribution of $H_0$  
3. Calculate p-value  
4. Make a Decision and a Conclusion.

